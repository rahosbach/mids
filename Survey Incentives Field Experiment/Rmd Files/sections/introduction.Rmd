\allsectionsfont{}
# Introduction

A wealth of existing research shows that using monetary incentives can increase survey response rates [see @armstrong_monetary_1975; @kanuk_mail_1975; and @duncan_mail_1979]. @james_large_1992 found that mailing an incentive of only \$1 with the survey significantly increased the response rate compared to people who received no incentive.  More recently, @debell_improving_2020 found that including \$5 of visible money in a mailed survey increased response rates from roughly 43% to 47%.  Researchers have also studied response rates for surveys administered to college students.  For instance, @szelenyi_what_2005 found that a \$2 incentive led to increased response rates among college students compared to no incentive.

This body of research suggests that direct financial incentives can lead to higher survey response rates.  But, while researchers have studied direct financial incentives in depth, the effect of indirect, philanthropic donations are comparatively less well understood.  @gattellari_will_2001 found that offering donations to Australian surgeons' alma maters actually led to lower response rates when compared to offering no incentive at all.  @nesrallah_charitable_2014 and @warwick_small_2019 had similar findings in medical-related settings.  Outside of the medical field, @pedersen_improving_2016 also found that incentives promising donations to a good cause led to decreased response rates relative to no incentives in online surveys.

We conducted a field experiment to test the effect of two different incentives on the survey response rates from alumni of UC Berkeley's Master of Information and Data Science (MIDS) program.  One incentive (hereafter referred to as the "direct incentive") provides a chance to win a \$25 Amazon gift card if the alumnus completes the survey.  This direct financial incentive is akin to the direct financial incentives discussed above.  The other incentive (hereafter referred to as the "philanthropic incentive") provides an opportunity to have us, the survey developers, donate funds to the Berkeley Student Food Collective (BSFC) if we achieve a sufficiently high survey response rate.  While some studies found donation-based incentives counterproductive, our experiment takes a slightly different approach.  Instead of promising a donation to BSFC for an individualâ€™s response, we will test the effect of survey response rates when we tell individuals that the donation will only occur if we meet a certain threshold of _group_ response rate.

In this experiment, we evaluated the effects of the direct and philanthropic incentives to cause higher survey response rates when compared to a control group that received no incentive.  We gave all three groups the same survey that we distributed via one of our personal email addresses; however, only alumni in the direct and philanthropic incentive groups received text in their emails regarding the incentive assigned to them.  We hypothesized that both the direct and philanthropic incentives would cause higher survey response rates among the MIDS alumni.  As mentioned, direct incentives have led to increased survey response rates in certain contexts, whereas donation-based incentives have shown far less promise.  Nevertheless, we still expected the philanthropic incentive to garner higher survey response rates relative to the control group for the following reasons: 1) a group incentive, when compared to a personal incentive, may prove more inherently motivating for some people; 2) people may prefer the certainty of the donation to charity after reaching the response threshold over the uncertainty of receiving a personal monetary reward; and 3) trying to help a good cause can evoke a sense of meaning when compared to receiving money personally.

In the following section we describe our methodology for conducting this experiment, including the tools we used, our data collection process, how we dealt with attrition and non-compliance, and other details.  Next, we report our experimental results and provide a discussion of their meaning and generalizability.  We also discuss potential follow-on opportunities from this experiment.  Finally, we conclude our paper and provide a series of appendices containing the email text we used, Python and R code we used for data collection and analysis, the actual survey, and a summary of the survey responses we received.
